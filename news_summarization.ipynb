{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:/Users/ishaan phaye/Desktop/Data files/news.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.content=df.content.astype(str)\n",
    "df['Original Content']=df['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['content'] = df['content'].replace('\\n', ' ')\n",
    "df['content'] = df['content'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['word_count'] = df['content'].apply(lambda x: len(x.split()))\n",
    "df['sentence_count'] = df['content'].apply(lambda x: len(x.split('. ')))\n",
    "df['avg_sentence_length'] = df['word_count'] / df['sentence_count']\n",
    "df['avg_word_length'] = df['content'].apply(lambda x: np.mean([len(word) for word in x.split()]))\n",
    "df['Original Metrics'] = df.apply(lambda x: f\"Word Count: {x['word_count']}, Sentence Count: {x['sentence_count']}, Avg Sentence Length: {x['avg_sentence_length']:.2f}, Avg Word Length: {x['avg_word_length']:.2f}\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test-train split of 10-90\n",
    "train_set = df.sample(frac=0.9, random_state=42)\n",
    "test_set = df.drop(train_set.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set['content'] = train_set['content'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = vectorizer.fit_transform(train_set['content'])\n",
    "similarity_matrix = cosine_similarity(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_responses = []\n",
    "for index, row in test_set.iterrows():\n",
    "    sentence_scores = {}\n",
    "    sentences = str(row['content']).split('. ')\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        sentence_scores[i] = 0\n",
    "        for j, similarity_score in enumerate(similarity_matrix[i]):\n",
    "            if i == j:\n",
    "                continue\n",
    "            sentence_scores[i] += similarity_score\n",
    "    top_sentences = sorted(sentence_scores.items(), key=lambda x: x[1], reverse=True)[:int(len(sentences) * 0.3)]\n",
    "    top_sentences = sorted(top_sentences, key=lambda x: x[0])\n",
    "    cleaned_sentences = []\n",
    "    removed_lines = []\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        if i in [index for index, score in top_sentences]:\n",
    "            cleaned_sentences.append(sentence)\n",
    "        else:\n",
    "            removed_lines.append(sentence)\n",
    "    cleaned_responses.append('. '.join(cleaned_sentences))\n",
    "    test_set.at[index, 'Removed Lines'] = '. '.join(removed_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set['New Content'] = cleaned_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional metrics for reduced data\n",
    "test_set['word_count'] = test_set['New Content'].apply(lambda x: len(x.split()))\n",
    "test_set['sentence_count'] = test_set['New Content'].apply(lambda x: len(x.split('. ')))\n",
    "test_set['avg_sentence_length'] = test_set['word_count'] / test_set['sentence_count']\n",
    "test_set['avg_word_length'] = test_set['New Content'].apply(lambda x: np.mean([len(word) for word in x.split()]))\n",
    "test_set['New Metrics'] = test_set.apply(lambda x: f\"Word Count: {x['word_count']}, Sentence Count: {x['sentence_count']}, Avg Sentence Length: {x['avg_sentence_length']:.2f}, Avg Word Length: {x['avg_word_length']:.2f}\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = test_set[['Original Content','Original Metrics', 'New Content', 'New Metrics','Removed Lines']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run this to output the test_set.csv file\n",
    "# test_df = pd.DataFrame(test_set)\n",
    "# test_df.to_csv('test_cleaned_responses.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
